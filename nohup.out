Training on Normal Data
W0815 14:42:22.119000 140440217372288 torch/distributed/run.py:757] 
W0815 14:42:22.119000 140440217372288 torch/distributed/run.py:757] *****************************************
W0815 14:42:22.119000 140440217372288 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0815 14:42:22.119000 140440217372288 torch/distributed/run.py:757] *****************************************
[2025-08-15 14:42:26,671] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 14:42:26,707] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 14:42:26,745] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 14:42:26,778] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 14:42:26,812] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 14:42:26,846] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-08-15 14:42:26,881] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-08-15 14:42:26,924] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-08-15 14:42:26,967] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 14:42:26,971] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:34,962 INFO training on multiple gpus, this gpu 8, rank 8, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:34,971 INFO training on multiple gpus, this gpu 1, rank 1, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:34,974 INFO training on multiple gpus, this gpu 4, rank 4, world_size 10
2025-08-15 14:42:34,983 INFO training on multiple gpus, this gpu 2, rank 2, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:34,986 INFO training on multiple gpus, this gpu 7, rank 7, world_size 10
2025-08-15 14:42:34,998 INFO training on multiple gpus, this gpu 3, rank 3, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:35,049 INFO training on multiple gpus, this gpu 0, rank 0, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:35,158 INFO training on multiple gpus, this gpu 5, rank 5, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:35,241 INFO training on multiple gpus, this gpu 9, rank 9, world_size 10
/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/dataset/processor.py:26: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
2025-08-15 14:42:35,299 INFO training on multiple gpus, this gpu 6, rank 6, world_size 10
2025-08-15 14:42:53,158 INFO [Rank 0] Checkpoint: save to checkpoint /tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/exp/cosyvoice/llm/CosyVoice-300M-KO_224h/torch_ddp/init.pt
2025-08-15 14:42:53,210 INFO Epoch 0 TRAIN info lr 1e-05 rank 0
2025-08-15 14:42:53,210 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,217 INFO Epoch 0 TRAIN info lr 1e-05 rank 4
2025-08-15 14:42:53,217 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,217 INFO Epoch 0 TRAIN info lr 1e-05 rank 8
2025-08-15 14:42:53,217 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,217 INFO Epoch 0 TRAIN info lr 1e-05 rank 3
2025-08-15 14:42:53,217 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,265 INFO Epoch 0 TRAIN info lr 1e-05 rank 1
2025-08-15 14:42:53,266 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,266 INFO Epoch 0 TRAIN info lr 1e-05 rank 2
2025-08-15 14:42:53,266 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,372 INFO Epoch 0 TRAIN info lr 1e-05 rank 9
2025-08-15 14:42:53,372 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,374 INFO Epoch 0 TRAIN info lr 1e-05 rank 6
2025-08-15 14:42:53,374 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,375 INFO Epoch 0 TRAIN info lr 1e-05 rank 5
2025-08-15 14:42:53,375 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:42:53,376 INFO Epoch 0 TRAIN info lr 1e-05 rank 7
2025-08-15 14:42:53,376 INFO using accumulate grad, new batch size is 200 times larger than before
2025-08-15 14:43:41,790 DEBUG TRAIN Batch 0/100 loss 0.015729 acc 0.266129 lr 0.00001000 grad_norm 0.000000 rank 2
2025-08-15 14:43:41,803 DEBUG TRAIN Batch 0/100 loss 0.014154 acc 0.325301 lr 0.00001000 grad_norm 0.000000 rank 1
2025-08-15 14:43:41,804 DEBUG TRAIN Batch 0/100 loss 0.017305 acc 0.283186 lr 0.00001000 grad_norm 0.000000 rank 0
2025-08-15 14:43:41,806 DEBUG TRAIN Batch 0/100 loss 0.013530 acc 0.316239 lr 0.00001000 grad_norm 0.000000 rank 8
2025-08-15 14:43:41,808 DEBUG TRAIN Batch 0/100 loss 0.015660 acc 0.234043 lr 0.00001000 grad_norm 0.000000 rank 4
2025-08-15 14:43:41,809 DEBUG TRAIN Batch 0/100 loss 0.013576 acc 0.316327 lr 0.00001000 grad_norm 0.000000 rank 6
2025-08-15 14:43:41,810 DEBUG TRAIN Batch 0/100 loss 0.014652 acc 0.352941 lr 0.00001000 grad_norm 0.000000 rank 5
2025-08-15 14:43:41,812 DEBUG TRAIN Batch 0/100 loss 0.016185 acc 0.317829 lr 0.00001000 grad_norm 0.000000 rank 9
2025-08-15 14:43:41,821 DEBUG TRAIN Batch 0/100 loss 0.014268 acc 0.279851 lr 0.00001000 grad_norm 0.000000 rank 3
2025-08-15 14:43:41,825 DEBUG TRAIN Batch 0/100 loss 0.017042 acc 0.242424 lr 0.00001000 grad_norm 0.000000 rank 7
[rank7]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2025-08-15 14:44:03,894 DEBUG TRAIN Batch 0/200 loss 0.015017 acc 0.360294 lr 0.00001000 grad_norm 2.872103 rank 2
2025-08-15 14:44:03,895 DEBUG TRAIN Batch 0/200 loss 0.017222 acc 0.239437 lr 0.00001000 grad_norm 2.872103 rank 9
2025-08-15 14:44:03,896 DEBUG TRAIN Batch 0/200 loss 0.016250 acc 0.284404 lr 0.00001000 grad_norm 2.872103 rank 7
2025-08-15 14:44:03,933 DEBUG TRAIN Batch 0/200 loss 0.014290 acc 0.338583 lr 0.00001000 grad_norm 2.872103 rank 8
2025-08-15 14:44:03,946 DEBUG TRAIN Batch 0/200 loss 0.016694 acc 0.307087 lr 0.00001000 grad_norm 2.872103 rank 0
2025-08-15 14:44:03,948 DEBUG TRAIN Batch 0/200 loss 0.014500 acc 0.284916 lr 0.00001000 grad_norm 2.872103 rank 1
2025-08-15 14:44:03,951 DEBUG TRAIN Batch 0/200 loss 0.014714 acc 0.231511 lr 0.00001000 grad_norm 2.872103 rank 3
2025-08-15 14:44:03,953 DEBUG TRAIN Batch 0/200 loss 0.013503 acc 0.388889 lr 0.00001000 grad_norm 2.872103 rank 5
2025-08-15 14:44:03,983 DEBUG TRAIN Batch 0/200 loss 0.014749 acc 0.292453 lr 0.00001000 grad_norm 2.872103 rank 6
2025-08-15 14:44:04,004 DEBUG TRAIN Batch 0/200 loss 0.012701 acc 0.343137 lr 0.00001000 grad_norm 2.872103 rank 4
2025-08-15 14:44:24,359 DEBUG TRAIN Batch 0/300 loss 0.013122 acc 0.312500 lr 0.00001000 grad_norm 0.000000 rank 3
2025-08-15 14:44:24,362 DEBUG TRAIN Batch 0/300 loss 0.015004 acc 0.282759 lr 0.00001000 grad_norm 0.000000 rank 2
2025-08-15 14:44:24,363 DEBUG TRAIN Batch 0/300 loss 0.014885 acc 0.324561 lr 0.00001000 grad_norm 0.000000 rank 5
2025-08-15 14:44:24,367 DEBUG TRAIN Batch 0/300 loss 0.015428 acc 0.296296 lr 0.00001000 grad_norm 0.000000 rank 1
2025-08-15 14:44:24,369 DEBUG TRAIN Batch 0/300 loss 0.013704 acc 0.318584 lr 0.00001000 grad_norm 0.000000 rank 6
2025-08-15 14:44:24,374 DEBUG TRAIN Batch 0/300 loss 0.015217 acc 0.250000 lr 0.00001000 grad_norm 0.000000 rank 8
2025-08-15 14:44:24,376 DEBUG TRAIN Batch 0/300 loss 0.014488 acc 0.320261 lr 0.00001000 grad_norm 0.000000 rank 9
2025-08-15 14:44:24,378 DEBUG TRAIN Batch 0/300 loss 0.015036 acc 0.286765 lr 0.00001000 grad_norm 0.000000 rank 0
2025-08-15 14:44:24,381 DEBUG TRAIN Batch 0/300 loss 0.016173 acc 0.327586 lr 0.00001000 grad_norm 0.000000 rank 7
2025-08-15 14:44:24,397 DEBUG TRAIN Batch 0/300 loss 0.015081 acc 0.297297 lr 0.00001000 grad_norm 0.000000 rank 4
2025-08-15 14:44:46,092 DEBUG TRAIN Batch 0/400 loss 0.014751 acc 0.327869 lr 0.00001000 grad_norm 2.192930 rank 5
2025-08-15 14:44:46,093 DEBUG TRAIN Batch 0/400 loss 0.014894 acc 0.344828 lr 0.00001000 grad_norm 2.192930 rank 0
2025-08-15 14:44:46,092 DEBUG TRAIN Batch 0/400 loss 0.015633 acc 0.296482 lr 0.00001000 grad_norm 2.192930 rank 1
2025-08-15 14:44:46,092 DEBUG TRAIN Batch 0/400 loss 0.013056 acc 0.319444 lr 0.00001000 grad_norm 2.192930 rank 8
2025-08-15 14:44:46,092 DEBUG TRAIN Batch 0/400 loss 0.015703 acc 0.209150 lr 0.00001000 grad_norm 2.192930 rank 2
2025-08-15 14:44:46,092 DEBUG TRAIN Batch 0/400 loss 0.014267 acc 0.279330 lr 0.00001000 grad_norm 2.192930 rank 3
2025-08-15 14:44:46,092 DEBUG TRAIN Batch 0/400 loss 0.015594 acc 0.306748 lr 0.00001000 grad_norm 2.192930 rank 9
2025-08-15 14:44:46,093 DEBUG TRAIN Batch 0/400 loss 0.013419 acc 0.366667 lr 0.00001000 grad_norm 2.192930 rank 6
2025-08-15 14:44:46,093 DEBUG TRAIN Batch 0/400 loss 0.014324 acc 0.271186 lr 0.00001000 grad_norm 2.192930 rank 4
2025-08-15 14:44:46,094 DEBUG TRAIN Batch 0/400 loss 0.016144 acc 0.243902 lr 0.00001000 grad_norm 2.192930 rank 7
2025-08-15 14:45:06,154 DEBUG TRAIN Batch 0/500 loss 0.013182 acc 0.331593 lr 0.00001000 grad_norm 0.000000 rank 3
2025-08-15 14:45:06,156 DEBUG TRAIN Batch 0/500 loss 0.015224 acc 0.254808 lr 0.00001000 grad_norm 0.000000 rank 1
2025-08-15 14:45:06,162 DEBUG TRAIN Batch 0/500 loss 0.012876 acc 0.307190 lr 0.00001000 grad_norm 0.000000 rank 8
2025-08-15 14:45:06,165 DEBUG TRAIN Batch 0/500 loss 0.012122 acc 0.385621 lr 0.00001000 grad_norm 0.000000 rank 0
2025-08-15 14:45:06,173 DEBUG TRAIN Batch 0/500 loss 0.013399 acc 0.304688 lr 0.00001000 grad_norm 0.000000 rank 5
2025-08-15 14:45:06,173 DEBUG TRAIN Batch 0/500 loss 0.014316 acc 0.319018 lr 0.00001000 grad_norm 0.000000 rank 2
2025-08-15 14:45:06,176 DEBUG TRAIN Batch 0/500 loss 0.013830 acc 0.372093 lr 0.00001000 grad_norm 0.000000 rank 9
2025-08-15 14:45:06,176 DEBUG TRAIN Batch 0/500 loss 0.014722 acc 0.265625 lr 0.00001000 grad_norm 0.000000 rank 6
2025-08-15 14:45:06,178 DEBUG TRAIN Batch 0/500 loss 0.012906 acc 0.361538 lr 0.00001000 grad_norm 0.000000 rank 7
2025-08-15 14:45:06,181 DEBUG TRAIN Batch 0/500 loss 0.013527 acc 0.328000 lr 0.00001000 grad_norm 0.000000 rank 4
2025-08-15 14:45:28,067 DEBUG TRAIN Batch 0/600 loss 0.013992 acc 0.294479 lr 0.00001000 grad_norm 1.585402 rank 8
2025-08-15 14:45:28,068 DEBUG TRAIN Batch 0/600 loss 0.012347 acc 0.341404 lr 0.00001000 grad_norm 1.585402 rank 3
2025-08-15 14:45:28,067 DEBUG TRAIN Batch 0/600 loss 0.013977 acc 0.307339 lr 0.00001000 grad_norm 1.585402 rank 1
2025-08-15 14:45:28,067 DEBUG TRAIN Batch 0/600 loss 0.014330 acc 0.281481 lr 0.00001000 grad_norm 1.585402 rank 5
2025-08-15 14:45:28,067 DEBUG TRAIN Batch 0/600 loss 0.015834 acc 0.298343 lr 0.00001000 grad_norm 1.585402 rank 9
2025-08-15 14:45:28,067 DEBUG TRAIN Batch 0/600 loss 0.015016 acc 0.287356 lr 0.00001000 grad_norm 1.585402 rank 2
2025-08-15 14:45:28,067 DEBUG TRAIN Batch 0/600 loss 0.014511 acc 0.350365 lr 0.00001000 grad_norm 1.585402 rank 7
2025-08-15 14:45:28,068 DEBUG TRAIN Batch 0/600 loss 0.014708 acc 0.310976 lr 0.00001000 grad_norm 1.585402 rank 0
2025-08-15 14:45:28,068 DEBUG TRAIN Batch 0/600 loss 0.013775 acc 0.318182 lr 0.00001000 grad_norm 1.585402 rank 4
2025-08-15 14:45:28,069 DEBUG TRAIN Batch 0/600 loss 0.014489 acc 0.259259 lr 0.00001000 grad_norm 1.585402 rank 6
2025-08-15 14:45:48,476 DEBUG TRAIN Batch 0/700 loss 0.014252 acc 0.289773 lr 0.00001000 grad_norm 0.000000 rank 0
2025-08-15 14:45:48,486 DEBUG TRAIN Batch 0/700 loss 0.014709 acc 0.306878 lr 0.00001000 grad_norm 0.000000 rank 2
2025-08-15 14:45:48,493 DEBUG TRAIN Batch 0/700 loss 0.014420 acc 0.307692 lr 0.00001000 grad_norm 0.000000 rank 8
2025-08-15 14:45:48,492 DEBUG TRAIN Batch 0/700 loss 0.017539 acc 0.206349 lr 0.00001000 grad_norm 0.000000 rank 9
2025-08-15 14:45:48,495 DEBUG TRAIN Batch 0/700 loss 0.012657 acc 0.347222 lr 0.00001000 grad_norm 0.000000 rank 6
2025-08-15 14:45:48,499 DEBUG TRAIN Batch 0/700 loss 0.013493 acc 0.282759 lr 0.00001000 grad_norm 0.000000 rank 5
2025-08-15 14:45:48,501 DEBUG TRAIN Batch 0/700 loss 0.013485 acc 0.316901 lr 0.00001000 grad_norm 0.000000 rank 4
2025-08-15 14:45:48,509 DEBUG TRAIN Batch 0/700 loss 0.011624 acc 0.449339 lr 0.00001000 grad_norm 0.000000 rank 1
2025-08-15 14:45:48,514 DEBUG TRAIN Batch 0/700 loss 0.013468 acc 0.361419 lr 0.00001000 grad_norm 0.000000 rank 3
2025-08-15 14:45:48,525 DEBUG TRAIN Batch 0/700 loss 0.012507 acc 0.344828 lr 0.00001000 grad_norm 0.000000 rank 7
2025-08-15 14:46:10,279 DEBUG TRAIN Batch 0/800 loss 0.012448 acc 0.359813 lr 0.00001000 grad_norm 1.177305 rank 2
2025-08-15 14:46:10,279 DEBUG TRAIN Batch 0/800 loss 0.014036 acc 0.335484 lr 0.00001000 grad_norm 1.177305 rank 5
2025-08-15 14:46:10,279 DEBUG TRAIN Batch 0/800 loss 0.014061 acc 0.264151 lr 0.00001000 grad_norm 1.177305 rank 6
2025-08-15 14:46:10,279 DEBUG TRAIN Batch 0/800 loss 0.013938 acc 0.281690 lr 0.00001000 grad_norm 1.177305 rank 8
2025-08-15 14:46:10,279 DEBUG TRAIN Batch 0/800 loss 0.013759 acc 0.402597 lr 0.00001000 grad_norm 1.177305 rank 4
2025-08-15 14:46:10,279 DEBUG TRAIN Batch 0/800 loss 0.014112 acc 0.336634 lr 0.00001000 grad_norm 1.177305 rank 9
2025-08-15 14:46:10,280 DEBUG TRAIN Batch 0/800 loss 0.014718 acc 0.259740 lr 0.00001000 grad_norm 1.177305 rank 7
2025-08-15 14:46:10,281 DEBUG TRAIN Batch 0/800 loss 0.013757 acc 0.291139 lr 0.00001000 grad_norm 1.177305 rank 1
2025-08-15 14:46:10,281 DEBUG TRAIN Batch 0/800 loss 0.013146 acc 0.347222 lr 0.00001000 grad_norm 1.177305 rank 3
2025-08-15 14:46:10,282 DEBUG TRAIN Batch 0/800 loss 0.013507 acc 0.329787 lr 0.00001000 grad_norm 1.177305 rank 0
2025-08-15 14:46:31,285 DEBUG TRAIN Batch 0/900 loss 0.013012 acc 0.317919 lr 0.00001000 grad_norm 0.000000 rank 6
2025-08-15 14:46:31,293 DEBUG TRAIN Batch 0/900 loss 0.013121 acc 0.358025 lr 0.00001000 grad_norm 0.000000 rank 8
2025-08-15 14:46:31,294 DEBUG TRAIN Batch 0/900 loss 0.013300 acc 0.314286 lr 0.00001000 grad_norm 0.000000 rank 4
2025-08-15 14:46:31,304 DEBUG TRAIN Batch 0/900 loss 0.015664 acc 0.244318 lr 0.00001000 grad_norm 0.000000 rank 5
2025-08-15 14:46:31,311 DEBUG TRAIN Batch 0/900 loss 0.013932 acc 0.351759 lr 0.00001000 grad_norm 0.000000 rank 0
2025-08-15 14:46:31,316 DEBUG TRAIN Batch 0/900 loss 0.015208 acc 0.300000 lr 0.00001000 grad_norm 0.000000 rank 7
2025-08-15 14:46:31,316 DEBUG TRAIN Batch 0/900 loss 0.014621 acc 0.316742 lr 0.00001000 grad_norm 0.000000 rank 9
2025-08-15 14:46:31,317 DEBUG TRAIN Batch 0/900 loss 0.012169 acc 0.379447 lr 0.00001000 grad_norm 0.000000 rank 1
2025-08-15 14:46:31,336 DEBUG TRAIN Batch 0/900 loss 0.010388 acc 0.466667 lr 0.00001000 grad_norm 0.000000 rank 3
2025-08-15 14:46:31,346 DEBUG TRAIN Batch 0/900 loss 0.014361 acc 0.315574 lr 0.00001000 grad_norm 0.000000 rank 2
[rank3]: Traceback (most recent call last):
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/bin/train.py", line 158, in <module>
[rank3]:     main()
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/bin/train.py", line 154, in main
[rank3]:     executor.train_one_epoc(model, optimizer, scheduler, train_data_loader, cv_data_loader, writer, info_dict, scaler, group_join)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/utils/executor.py", line 68, in train_one_epoc
[rank3]:     info_dict = batch_forward(model, batch_dict, scaler, info_dict)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/utils/train_utils.py", line 248, in batch_forward
[rank3]:     info_dict['loss_dict'] = model(batch, device)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/llm/llm.py", line 180, in forward
[rank3]:     lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/encoder.py", line 160, in forward
[rank3]:     xs = self.forward_layers(xs, chunk_masks, pos_emb, mask_pad)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/encoder.py", line 172, in forward_layers
[rank3]:     xs, chunk_masks, _, _ = layer(xs, chunk_masks, pos_emb, mask_pad)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/encoder_layer.py", line 94, in forward
[rank3]:     x_att, new_att_cache = self.self_attn(x, x, x, mask, pos_emb=pos_emb, cache=att_cache)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/attention.py", line 329, in forward
[rank3]:     return self.forward_attention(v, scores, mask), new_cache
[rank3]:   File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/attention.py", line 114, in forward_attention
[rank3]:     attn = torch.softmax(scores, dim=-1).masked_fill(
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 122.00 MiB. GPU  has a total capacity of 10.75 GiB of which 79.56 MiB is free. Including non-PyTorch memory, this process has 10.67 GiB memory in use. Of the allocated memory 10.27 GiB is allocated by PyTorch, and 128.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-15 14:46:58,331 DEBUG Attempting to acquire lock 140545842998256 on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-15 14:46:58,332 DEBUG Lock 140545842998256 acquired on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-15 14:46:58,333 DEBUG Attempting to release lock 140545842998256 on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-15 14:46:58,333 DEBUG Lock 140545842998256 released on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-15 14:46:58,341 DEBUG Attempting to acquire lock 140545842998304 on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2025-08-15 14:46:58,341 DEBUG Lock 140545842998304 acquired on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2025-08-15 14:46:58,342 DEBUG Attempting to release lock 140545842998304 on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2025-08-15 14:46:58,342 DEBUG Lock 140545842998304 released on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
[rank0]:[E ProcessGroupGloo.cpp:144] [Rank 0]: Rank 3 failed to pass monitoredBarrier in 60000 ms
2025-08-15 14:46:59,329 INFO Detected uneven workload distribution: [Rank 0]: Rank 3 failed to pass monitoredBarrier in 60000 ms
 Original exception: 
[../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [::1]:47830
Break current worker to manually join all workers, world_size 10, current rank 0, current local_rank 0

W0815 14:47:02.692000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46114 closing signal SIGTERM
W0815 14:47:02.693000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46115 closing signal SIGTERM
W0815 14:47:02.694000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46116 closing signal SIGTERM
W0815 14:47:02.695000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46118 closing signal SIGTERM
W0815 14:47:02.696000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46119 closing signal SIGTERM
W0815 14:47:02.697000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46120 closing signal SIGTERM
W0815 14:47:02.698000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46121 closing signal SIGTERM
W0815 14:47:02.698000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46122 closing signal SIGTERM
W0815 14:47:02.699000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 46123 closing signal SIGTERM
E0815 14:47:04.519000 140440217372288 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 3 (pid: 46117) of binary: /work/anaconda/envs/dhy_marco_voice/bin/python3.10
E0815 14:47:04.537000 140440217372288 torch/distributed/elastic/multiprocessing/errors/error_handler.py:136] no error file defined for parent, to copy child error file (/tmp/torchelastic_36cod1qf/1986_zj6m6l_2/attempt_0/3/error.json)
Traceback (most recent call last):
  File "/work/anaconda/envs/dhy_marco_voice/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
cosyvoice_rodis/bin/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-15_14:46:58
  host      : localhost.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 46117)
  error_file: /tmp/torchelastic_36cod1qf/1986_zj6m6l_2/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
      return f(*args, **kwargs)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/bin/train.py", line 154, in main
      executor.train_one_epoc(model, optimizer, scheduler, train_data_loader, cv_data_loader, writer, info_dict, scaler, group_join)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/utils/executor.py", line 68, in train_one_epoc
      info_dict = batch_forward(model, batch_dict, scaler, info_dict)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/utils/train_utils.py", line 248, in batch_forward
      info_dict['loss_dict'] = model(batch, device)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
      return forward_call(*args, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
      else self._run_ddp_forward(*inputs, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
      return self.module(*inputs, **kwargs)  # type: ignore[index]
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
      return forward_call(*args, **kwargs)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/llm/llm.py", line 180, in forward
      lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
      return forward_call(*args, **kwargs)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/encoder.py", line 160, in forward
      xs = self.forward_layers(xs, chunk_masks, pos_emb, mask_pad)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/encoder.py", line 172, in forward_layers
      xs, chunk_masks, _, _ = layer(xs, chunk_masks, pos_emb, mask_pad)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
      return forward_call(*args, **kwargs)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/encoder_layer.py", line 94, in forward
      x_att, new_att_cache = self.self_attn(x, x, x, mask, pos_emb=pos_emb, cache=att_cache)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/work/anaconda/envs/dhy_marco_voice/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
      return forward_call(*args, **kwargs)
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/attention.py", line 329, in forward
      return self.forward_attention(v, scores, mask), new_cache
    File "/tsdata2/dhy/tts/Marco-Voice/Models/marco_voice/cosyvoice_rodis/transformer/attention.py", line 114, in forward_attention
      attn = torch.softmax(scores, dim=-1).masked_fill(
  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 122.00 MiB. GPU  has a total capacity of 10.75 GiB of which 79.56 MiB is free. Including non-PyTorch memory, this process has 10.67 GiB memory in use. Of the allocated memory 10.27 GiB is allocated by PyTorch, and 128.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================
